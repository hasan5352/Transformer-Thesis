{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f574bd",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- [Download corrupted Cifar10 .tar](https://zenodo.org/records/2535967) and extract it into a folder.\n",
    "\n",
    "- Cifar10 is downloaded using torch.datasets\n",
    "\n",
    "- [Download corrupted Tiny-ImageNet](https://zenodo.org/records/2536630) and extract it into a folder.\n",
    "\n",
    "- [Download Tiny-ImageNet](https://www.kaggle.com/datasets/akash2sharma/tiny-imagenet) and extract it into a folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b7c4a",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1ea522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Hp\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timm\n",
    "\n",
    "from my_transformers import CorruptDistillVisionTransformer\n",
    "from utils import load_experimental_TinyImageNet\n",
    "from train_test_module import compute_ece\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ed7d8",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ec91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_types = [\"motion_blur\", \"shot_noise\", \"jpeg_compression\", \"fog\"]\n",
    "\n",
    "# corrupt_path = r\"C:\\Users\\Hp\\Desktop\\Coding\\Transformer-Thesis\\Tiny-ImageNet-C\\Tiny-ImageNet-C\"\n",
    "# normal_path = r\"C:\\Users\\Hp\\Desktop\\Coding\\Transformer-Thesis\\Tiny-ImageNet-Normal\"\n",
    "# train_data, test_data = load_experimental_TinyImageNet(normal_path, corrupt_path, corrupt_types, num_train_imgs=20)\n",
    "\n",
    "# torch.save(train_data, \"train_data.pt\")\n",
    "# torch.save(test_data, \"test_data.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68f313",
   "metadata": {},
   "source": [
    "# Visualisation TinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_img(tensor):\n",
    "    mean = torch.tensor([0.4802, 0.4481, 0.3975])\n",
    "    std = torch.tensor([0.2302, 0.2265, 0.2262])\n",
    "    \n",
    "    # denormalize\n",
    "    img = tensor.clone()\n",
    "    img = img * std[:, None, None] + mean[:, None, None]\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    # convert to PIL image\n",
    "    to_pil = T.ToPILImage()\n",
    "    return to_pil(img)\n",
    "\n",
    "# plot\n",
    "imgs_to_display = [random.randint(0, len(train_data[0])-1) for i in range(9)]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(4, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    img, label = train_data[0][imgs_to_display[i]], train_data[2][imgs_to_display[i]].item()\n",
    "    img = tensor_to_img(img)\n",
    "    if label == len(corrupt_types):\n",
    "        axes[i].set_title(f\"normal\", fontsize=8)\n",
    "    else:\n",
    "        axes[i].set_title(corrupt_types[label], fontsize=8)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c65ee",
   "metadata": {},
   "source": [
    "# Model config for TinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e646c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed \n",
    "torch.cuda.manual_seed(22)\n",
    "random.seed(22)\n",
    "torch.manual_seed(22)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "corrupt_types = [\"motion_blur\", \"shot_noise\", \"jpeg_compression\", \"fog\"]\n",
    "\n",
    "# Hyper-parameters\n",
    "PATCH_SIZE = 8\n",
    "IMG_SIZE = 64\n",
    "EMBED_DIM = 192\n",
    "NUM_HEADS = 3\n",
    "NUM_IMG_TYPES = len(corrupt_types)+1\n",
    "NUM_ENCODERS = 12\n",
    "NUM_CLASSES = 200\n",
    "DROPOUT = 0\n",
    "DROP_PATH = 0.1\n",
    "ERASE_PROB = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2aaa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21751496\n",
      "Linear(in_features=384, out_features=200, bias=True)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_data = torch.load(\"train_data.pt\", weights_only=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(*torch.load(\"test_data.pt\", weights_only=True)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "itr = iter(test_loader)\n",
    "train_batch = [next(itr)]\n",
    "test_batch = [next(itr)]\n",
    "\n",
    "# deit3_small_patch16_224.fb_in22k_ft_in1k -- 22M\n",
    "# convnext_tiny.fb_in22k_ft_in1k -- 28M\n",
    "deit3_small = timm.create_model('deit3_small_patch16_224.fb_in22k_ft_in1k', pretrained=True).cuda()\n",
    "# teacher_model = timm.create_model('vit_small_patch16_224.augreg_in21k_ft_in1k', pretrained=True).cuda()\n",
    "deit3_small.head = nn.Linear(in_features=384, out_features=NUM_CLASSES, bias=True).cuda()\n",
    "num_params = sum(p.numel() for p in deit3_small.parameters())\n",
    "print(num_params)\n",
    "print(deit3_small.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24826aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LR Scheduler ===\n",
    "NUM_EPOCHS = 50\n",
    "WARMUP_EPOCHS = 3\n",
    "total_steps = NUM_EPOCHS * len(trai)\n",
    "warmup_steps = WARMUP_EPOCHS * len(train_batch)\n",
    "\n",
    "optimizer = optim.AdamW(deit3_small.parameters(), lr=5e-5, weight_decay=0.05)\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-5, total_iters=warmup_steps)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS-WARMUP_EPOCHS)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer, schedulers=[warmup_scheduler, lr_scheduler], milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "from train_test_module import MyAugments\n",
    "augmenter = MyAugments(NUM_CLASSES, mixup_p=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969c49a",
   "metadata": {},
   "source": [
    "# ------------------------ Baseline ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestBaseline:\n",
    "    def __init__(self, model:nn.Module, train_data, test_loader,\n",
    "                 augmenter, num_img_types, batch_size, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_data = train_data\n",
    "        self.test_loader = test_loader\n",
    "        self.num_img_types = num_img_types\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.augmenter = augmenter\n",
    "\n",
    "        self.all_test_metrics, self.all_train_metrics = [], []\n",
    "\n",
    "    def test(self, print_metrics=False):\n",
    "        top1_correct_preds, top5_correct_preds = 0, 0\n",
    "        total_samples, total_ece, total_entropy = 0, 0, 0\n",
    "\n",
    "        total_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top1_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top5_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch, c_batch in self.test_loader:\n",
    "                x_batch, y_batch, c_batch = x_batch.to(self.device), y_batch.to(self.device), c_batch.to(self.device)\n",
    "                x_batch = F.interpolate(x_batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                preds = self.model(x_batch)\n",
    "                del x_batch \n",
    "                \n",
    "                total_samples += y_batch.size(0)\n",
    "                # top-1 acc\n",
    "                top1_right = (torch.argmax(preds, dim=1) == y_batch)\n",
    "                top1_correct_preds += top1_right.sum().item()\n",
    "                # top-5 acc\n",
    "                top5_right = torch.topk(preds, 5, dim=1).indices.eq(y_batch.unsqueeze(1)).any(dim=1)\n",
    "                top5_correct_preds += top5_right.sum().item()\n",
    "                # ECE loss\n",
    "                total_ece += compute_ece(preds, y_batch, self.device)\n",
    "                # Per-type acc\n",
    "                for t in range(self.num_img_types):\n",
    "                    mask = (c_batch == t)\n",
    "                    total_per_type[t] += mask.sum()\n",
    "                    top1_correct_per_type[t] += top1_right[mask].sum()\n",
    "                    top5_correct_per_type[t] += top5_right[mask].sum()\n",
    "                # entropy\n",
    "                total_entropy += -torch.sum(torch.softmax(preds, dim=1) * torch.log_softmax(preds, dim=1), dim=1).sum().item()\n",
    "                del y_batch, c_batch\n",
    "                \n",
    "        top1_acc_per_type = (top1_correct_per_type / total_per_type).tolist()\n",
    "        test_metrics = {\n",
    "            \"top1_acc\" : top1_correct_preds / total_samples, \n",
    "            \"top5_acc\" : top5_correct_preds / total_samples, \n",
    "            \"top1_acc_per_type\" : top1_acc_per_type,\n",
    "            \"top5_acc_per_type\" :(top5_correct_per_type / total_per_type).tolist(), \n",
    "            \"error_rate_per_type\" : [1 - acc for acc in top1_acc_per_type],\n",
    "            \"ece\" : total_ece / total_samples,\n",
    "            \"entropy\" : total_entropy / total_samples\n",
    "        }\n",
    "        if print_metrics : print(f\"Test-Accuracy:{test_metrics['top1_acc']:.2f}\")\n",
    "        return test_metrics\n",
    "    \n",
    "    def train(self, optimizer, scheduler, save_path, \n",
    "              num_epochs=1, label_smoothing=0.1, print_metrics=False\n",
    "              ):\n",
    "        best_acc, epochs_no_improve, min_delta, patience = 0, 0, 0.003, 5\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            self.model.train()\n",
    "            print(f\"------- Epoch {epoch} -------\")\n",
    "            total_samples, top1_correct_preds, loss_total = 0, 0, 0\n",
    "            train_loader = DataLoader(dataset=TensorDataset(*self.train_data),batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            for x_batch, y_batch, _ in train_loader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                x_batch, y_batch = self.augmenter(x_batch, y_batch)\n",
    "                x_batch = F.interpolate(x_batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                preds = self.model(x_batch)\n",
    "\n",
    "                del x_batch                 # free memory\n",
    "                total_samples += y_batch.size(0)\n",
    "                # top-1 acc\n",
    "                top1_correct_preds += (torch.argmax(preds, dim=1) == y_batch).sum().item()\n",
    "                #loss\n",
    "                loss = F.cross_entropy(preds, y_batch, label_smoothing=label_smoothing)\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "\n",
    "                loss_total += loss.item() * y_batch.size(0)\n",
    "                del y_batch, preds, loss            # free memory\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            test_metrics = self.test()\n",
    "            train_metrics = {\n",
    "                \"loss_total\": loss_total/total_samples,\n",
    "                \"top1_acc\" : top1_correct_preds / total_samples\n",
    "                }\n",
    "            current_acc = test_metrics['top1_acc']\n",
    "            if print_metrics : \n",
    "                print(f\"train-loss: {train_metrics['loss_total']:.2f} -- train-acc: {train_metrics['top1_acc']:.2f} -- \"\n",
    "                      f\"train-acc: {current_acc:.2f}\"\n",
    "                      )\n",
    "            self.all_train_metrics.append(train_metrics)\n",
    "            self.all_test_metrics.append(test_metrics)\n",
    "            # early stopping\n",
    "            if current_acc - best_acc > min_delta:\n",
    "                best_acc = current_acc\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} — no improvement in {patience} epochs.\")\n",
    "                break\n",
    "        \n",
    "        # save trained model and metrics\n",
    "        if save_path:\n",
    "            torch.save(self.model.state_dict(), f\"{save_path}.pth\")\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "            \n",
    "            with open(f\"{save_path}_train_metrics.json\", \"w\") as f1:\n",
    "                json.dump(self.all_train_metrics, f1, indent=4)\n",
    "            with open(f\"{save_path}_test_metrics.json\", \"w\") as f2:\n",
    "                json.dump(self.all_test_metrics, f2, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cece0c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 -------\n"
     ]
    }
   ],
   "source": [
    "baseline_module = TrainTestBaseline(deit3_small, train_batch[0], test_batch, augmenter, NUM_IMG_TYPES, BATCH_SIZE, device)\n",
    "\n",
    "baseline_module.train(optimizer, scheduler, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8541180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_module import LossCalculatorDeiT\n",
    "class TrainTestDeiT:\n",
    "    def __init__(self, model:nn.Module, teacher_model:nn.Module, train_batches, test_batches, head_strategy, num_img_types,\n",
    "                 device\n",
    "                 ):\n",
    "        assert head_strategy > 0 and head_strategy <= 3\n",
    "        self.model = model.to(device)\n",
    "        self.teacher_model = teacher_model.to(device)\n",
    "        self.train_batches = train_batches\n",
    "        self.test_batches = test_batches\n",
    "        self.head_strategy = head_strategy\n",
    "        self.num_img_types = num_img_types\n",
    "        self.device = device\n",
    "\n",
    "        self.loss_calculater = LossCalculatorDeiT(self.teacher_model)\n",
    "        self.all_test_metrics, self.all_train_metrics = [], []\n",
    "    \n",
    "    # testing function\n",
    "    def test(self, print_metrics=False):\n",
    "        top1_correct_preds, top5_correct_preds = 0, 0\n",
    "        top1_correct_corruptions, total_samples, total_ece, total_entropy = 0, 0, 0, 0\n",
    "\n",
    "        total_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top1_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top5_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch, c_batch in self.test_batches:\n",
    "                x_batch, y_batch, c_batch = x_batch.to(self.device), y_batch.to(self.device), c_batch.to(self.device)\n",
    "                tokens = self.model(x_batch)\n",
    "                del x_batch\n",
    "                \n",
    "                if len(tokens) == 3 and self.head_strategy != 1:\n",
    "                    preds = (tokens[0] + tokens[1] + self.model.output_head.ffn(tokens[2])) / 3\n",
    "                else:\n",
    "                    preds = (tokens[0] + tokens[1]) / 2\n",
    "                \n",
    "                total_samples += y_batch.size(0)\n",
    "                # top-1 acc\n",
    "                top1_right = (torch.argmax(preds, dim=1) == y_batch)\n",
    "                top1_correct_preds += top1_right.sum().item()\n",
    "                # top-5 acc\n",
    "                top5_right = torch.topk(preds, 5, dim=1).indices.eq(y_batch.unsqueeze(1)).any(dim=1)\n",
    "                top5_correct_preds += top5_right.sum().item()\n",
    "                # ECE loss\n",
    "                total_ece += compute_ece(preds, y_batch, self.device)\n",
    "                # Per-type acc\n",
    "                for t in range(self.num_img_types):\n",
    "                    mask = (c_batch == t)\n",
    "                    total_per_type[t] += mask.sum()\n",
    "                    top1_correct_per_type[t] += top1_right[mask].sum()\n",
    "                    top5_correct_per_type[t] += top5_right[mask].sum()\n",
    "                # entropy\n",
    "                total_entropy += -torch.sum(torch.softmax(preds, dim=1) * torch.log_softmax(preds, dim=1), dim=1).sum().item()\n",
    "                # top-1 corruption classification acc\n",
    "                if len(tokens) == 3 : top1_correct_corruptions += (torch.argmax(tokens[2], dim=1) == c_batch).sum().item()              \n",
    "                del y_batch, c_batch, preds\n",
    "\n",
    "        top1_acc_per_type = (top1_correct_per_type / total_per_type).tolist()\n",
    "        test_metrics = {\n",
    "            \"top1_acc\" : top1_correct_preds / total_samples, \n",
    "            \"top5_acc\" : top5_correct_preds / total_samples, \n",
    "            \"top1_acc_per_type\" : top1_acc_per_type,\n",
    "            \"top5_acc_per_type\" :(top5_correct_per_type / total_per_type).tolist(), \n",
    "            \"error_rate_per_type\" : [1 - acc for acc in top1_acc_per_type],\n",
    "            \"ece\" : total_ece / total_samples,\n",
    "            \"entropy\" : total_entropy / total_samples\n",
    "        }\n",
    "        if top1_correct_corruptions : test_metrics[\"top1_corrupt_acc\"] = top1_correct_corruptions / total_samples\n",
    "        if print_metrics : print(f\"Test-Accuracy:{test_metrics['top1_acc']:.2f}\")\n",
    "        return test_metrics\n",
    "\n",
    "    # ------------ training function ------------\n",
    "    def train(self, optimizer, scheduler, save_path, num_epochs=1, print_metrics=False):\n",
    "        best_acc, epochs_no_improve, min_delta, patience = 0, 0, 0.003, 4\n",
    "        \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            self.model.train()\n",
    "            loss_total, loss_cls, loss_distill, loss_corrupt, loss_cls_corruptFFN = 0, 0, 0, 0, 0\n",
    "            total_samples, sim_cls_distill, sim_cls_corrupt, top1_correct_preds = 0, 0, 0, 0\n",
    "\n",
    "            print(f\"------- Epoch {epoch} -------\")\n",
    "            for x_batch, y_batch, c_batch in self.train_batches:\n",
    "                x_batch, y_batch, c_batch = x_batch.to(self.device), y_batch.to(self.device), c_batch.to(self.device)\n",
    "                tokens = self.model(x_batch)\n",
    "                \n",
    "                if len(tokens) == 3:\n",
    "                    losses = self.loss_calculater(tokens, (x_batch, y_batch, c_batch))\n",
    "                    if self.head_strategy >= 2: preds = (tokens[0] + tokens[1] + self.model.output_head.ffn(tokens[2])) / 3\n",
    "                    else: preds = (tokens[0] + tokens[1]) / 2\n",
    "                else:\n",
    "                    losses = self.loss_calculater(tokens, (x_batch, y_batch))\n",
    "                    preds = (tokens[0] + tokens[1]) / 2\n",
    "                \n",
    "                # top-1 acc\n",
    "                top1_correct_preds += (torch.argmax(preds, dim=1) == y_batch).sum().item()\n",
    "                \n",
    "                del x_batch, preds, c_batch\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                losses[0].backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                total_samples += y_batch.size(0)\n",
    "                # losses\n",
    "                loss_total += losses[0].item() * y_batch.size(0)\n",
    "                loss_cls += losses[1].item() * y_batch.size(0)\n",
    "                loss_distill += losses[2].item() * y_batch.size(0)\n",
    "                if len(losses) == 4:\n",
    "                    loss_corrupt += losses[3].item() * y_batch.size(0)\n",
    "                    sim_cls_corrupt += self.model.sim_cls_corrupt.item() * y_batch.size(0)\n",
    "                if len(losses) == 5:\n",
    "                    loss_cls_corruptFFN += losses[4].item() * y_batch.size(0)\n",
    "                # cosine similarity\n",
    "                sim_cls_distill += self.model.sim_cls_distill.item() * y_batch.size(0)\n",
    "\n",
    "                del y_batch\n",
    "                \n",
    "\n",
    "            train_metrics = {\n",
    "                \"top1_acc\" : top1_correct_preds / total_samples,\n",
    "                \"loss_total\": loss_total/total_samples,\n",
    "                \"loss_cls\": loss_cls/total_samples,\n",
    "                \"loss_distill\": loss_distill/total_samples,\n",
    "                \"sim_cls_distill\" : sim_cls_distill/total_samples,\n",
    "            }\n",
    "            if loss_corrupt: \n",
    "                train_metrics[\"loss_corrupt\"] = loss_corrupt/total_samples\n",
    "                train_metrics[\"sim_cls_corrupt\"] = sim_cls_corrupt/total_samples\n",
    "            if loss_cls_corruptFFN:\n",
    "                train_metrics[\"loss_cls_corruptFFN\"] = loss_cls_corruptFFN/total_samples\n",
    "            test_metrics = self.test()\n",
    "            \n",
    "            current_acc = test_metrics['top1_acc']\n",
    "            if print_metrics : \n",
    "                print(f\"train-loss: {train_metrics['loss_total']:.2f} -- train-acc: {train_metrics['top1_acc']:.2f} -- \"\n",
    "                      f\"train-acc: {current_acc:.2f}\"\n",
    "                      )\n",
    "            self.all_train_metrics.append(train_metrics)\n",
    "            self.all_test_metrics.append(test_metrics)\n",
    "\n",
    "            # early stopping\n",
    "            if current_acc - best_acc > min_delta:\n",
    "                best_acc = current_acc\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} — no improvement in {patience} epochs.\")\n",
    "                break\n",
    "        \n",
    "        # save trained model\n",
    "        if save_path:\n",
    "            torch.save(self.model.state_dict(), f\"{save_path}.pth\")\n",
    "            print(f\"\\nModel saved to {save_path}\")\n",
    "            with open(f\"{save_path}_train_metrics.json\", \"w\") as f1:\n",
    "                json.dump(self.all_train_metrics, f1, indent=4)\n",
    "            with open(f\"{save_path}_test_metrics.json\", \"w\") as f2:\n",
    "                json.dump(self.all_test_metrics, f2, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffd5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestCdeiT:\n",
    "    def __init__(self, model:nn.Module, teacher_model:nn.Module, train_batches, test_batches, head_strategy, num_img_types,\n",
    "                 device\n",
    "                 ):\n",
    "        assert head_strategy > 0 and head_strategy <= 3\n",
    "        self.model = model.to(device)\n",
    "        self.teacher_model = teacher_model.to(device)\n",
    "        self.train_batches = train_batches\n",
    "        self.test_batches = test_batches\n",
    "        self.head_strategy = head_strategy\n",
    "        self.num_img_types = num_img_types\n",
    "        self.device = device\n",
    "\n",
    "        self.loss_calculater = LossCalculatorDeiT(self.teacher_model)\n",
    "        self.all_test_metrics, self.all_train_metrics = [], []\n",
    "    \n",
    "    # testing function\n",
    "    def test(self, print_metrics=False):\n",
    "        top1_correct_preds, top5_correct_preds = 0, 0\n",
    "        top1_correct_corruptions, total_samples, total_ece, total_entropy = 0, 0, 0, 0\n",
    "\n",
    "        total_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top1_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "        top5_correct_per_type = torch.zeros(self.num_img_types, device=self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch, c_batch in self.test_batches:\n",
    "                x_batch, y_batch, c_batch = x_batch.to(self.device), y_batch.to(self.device), c_batch.to(self.device)\n",
    "                tokens = self.model(x_batch)\n",
    "                del x_batch\n",
    "                \n",
    "                if self.head_strategy >= 2:\n",
    "                    preds = (tokens[0] + tokens[1] + self.model.output_head.ffn(tokens[2])) / 3\n",
    "                else:\n",
    "                    preds = (tokens[0] + tokens[1]) / 2\n",
    "                \n",
    "                total_samples += y_batch.size(0)\n",
    "                # top-1 acc\n",
    "                top1_right = (torch.argmax(preds, dim=1) == y_batch)\n",
    "                top1_correct_preds += top1_right.sum().item()\n",
    "                # top-5 acc\n",
    "                top5_right = torch.topk(preds, 5, dim=1).indices.eq(y_batch.unsqueeze(1)).any(dim=1)\n",
    "                top5_correct_preds += top5_right.sum().item()\n",
    "                # ECE loss\n",
    "                total_ece += compute_ece(preds, y_batch, self.device)\n",
    "                # Per-type acc\n",
    "                for t in range(self.num_img_types):\n",
    "                    mask = (c_batch == t)\n",
    "                    total_per_type[t] += mask.sum()\n",
    "                    top1_correct_per_type[t] += top1_right[mask].sum()\n",
    "                    top5_correct_per_type[t] += top5_right[mask].sum()\n",
    "                # entropy\n",
    "                total_entropy += -torch.sum(torch.softmax(preds, dim=1) * torch.log_softmax(preds, dim=1), dim=1).sum().item()\n",
    "                # top-1 corruption classification acc\n",
    "                top1_correct_corruptions += (torch.argmax(tokens[2], dim=1) == c_batch).sum().item()              \n",
    "                del y_batch, c_batch, preds\n",
    "\n",
    "        top1_acc_per_type = (top1_correct_per_type / total_per_type).tolist()\n",
    "        test_metrics = {\n",
    "            \"top1_acc\" : top1_correct_preds / total_samples, \n",
    "            \"top5_acc\" : top5_correct_preds / total_samples, \n",
    "            \"top1_corrupt_acc\" : top1_correct_corruptions / total_samples,\n",
    "            \"top1_acc_per_type\" : top1_acc_per_type,\n",
    "            \"top5_acc_per_type\" :(top5_correct_per_type / total_per_type).tolist(), \n",
    "            \"error_rate_per_type\" : [1 - acc for acc in top1_acc_per_type],\n",
    "            \"ece\" : total_ece / total_samples,\n",
    "            \"entropy\" : total_entropy / total_samples\n",
    "        }\n",
    "        if print_metrics : print(f\"Test-Accuracy:{test_metrics['top1_acc']:.2f}\")\n",
    "        return test_metrics\n",
    "\n",
    "    # ------------ training function ------------\n",
    "    def train(self, optimizer, scheduler, save_path, num_epochs=1, print_metrics=False):\n",
    "\n",
    "        best_acc, epochs_no_improve, min_delta, patience = 0, 0, 0.003, 4\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            self.model.train()\n",
    "            loss_total, loss_cls, loss_distill, loss_corrupt, loss_cls_corruptFFN = 0, 0, 0, 0, 0\n",
    "            total_samples, sim_cls_distill, sim_cls_corrupt, top1_correct_preds = 0, 0, 0, 0\n",
    "\n",
    "            print(f\"------- Epoch {epoch} -------\")\n",
    "            for x_batch, y_batch, c_batch in self.train_batches:\n",
    "                x_batch, y_batch, c_batch = x_batch.to(self.device), y_batch.to(self.device), c_batch.to(self.device)\n",
    "                tokens = self.model(x_batch)\n",
    "                \n",
    "                losses = self.loss_calculater(tokens, (x_batch, y_batch, c_batch))\n",
    "                if self.head_strategy >= 2: preds = (tokens[0] + tokens[1] + self.model.output_head.ffn(tokens[2])) / 3\n",
    "                else: preds = (tokens[0] + tokens[1]) / 2\n",
    "                \n",
    "                # top-1 acc\n",
    "                top1_correct_preds += (torch.argmax(preds, dim=1) == y_batch).sum().item()\n",
    "                \n",
    "                del x_batch, preds, c_batch\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                losses[0].backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                total_samples += y_batch.size(0)\n",
    "                # losses\n",
    "                loss_total += losses[0].item() * y_batch.size(0)\n",
    "                loss_cls += losses[1].item() * y_batch.size(0)\n",
    "                loss_distill += losses[2].item() * y_batch.size(0)\n",
    "                loss_corrupt += losses[3].item() * y_batch.size(0)\n",
    "                if len(losses) == 5:\n",
    "                    loss_cls_corruptFFN += losses[4].item() * y_batch.size(0)\n",
    "                # cosine similarity\n",
    "                sim_cls_distill += self.model.sim_cls_distill.item() * y_batch.size(0)\n",
    "                sim_cls_corrupt += self.model.sim_cls_corrupt.item() * y_batch.size(0)\n",
    "                del y_batch\n",
    "                \n",
    "            train_metrics = {\n",
    "                \"top1_acc\" : top1_correct_preds / total_samples,\n",
    "                \"loss_total\": loss_total/total_samples,\n",
    "                \"loss_cls\": loss_cls/total_samples,\n",
    "                \"loss_distill\": loss_distill/total_samples,\n",
    "                \"sim_cls_distill\" : sim_cls_distill/total_samples,\n",
    "                \"loss_corrupt\" : loss_corrupt/total_samples,\n",
    "                \"sim_cls_corrupt\" : sim_cls_corrupt/total_samples\n",
    "            }\n",
    "            if loss_cls_corruptFFN : train_metrics[\"loss_cls_corruptFFN\"] = loss_cls_corruptFFN/total_samples\n",
    "            test_metrics = self.test()\n",
    "            \n",
    "            current_acc = test_metrics['top1_acc']\n",
    "            if print_metrics : \n",
    "                print(f\"train-loss: {train_metrics['loss_total']:.2f} -- train-acc: {train_metrics['top1_acc']:.2f} -- \"\n",
    "                      f\"train-acc: {current_acc:.2f}\"\n",
    "                      )\n",
    "            self.all_train_metrics.append(train_metrics)\n",
    "            self.all_test_metrics.append(test_metrics)\n",
    "\n",
    "            # early stopping\n",
    "            if current_acc - best_acc > min_delta:\n",
    "                best_acc = current_acc\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} — no improvement in {patience} epochs.\")\n",
    "                break\n",
    "        \n",
    "        # save trained model\n",
    "        if save_path:\n",
    "            torch.save(self.model.state_dict(), f\"{save_path}.pth\")\n",
    "            print(f\"\\nModel saved to {save_path}\")\n",
    "            with open(f\"{save_path}_train_metrics.json\", \"w\") as f1:\n",
    "                json.dump(self.all_train_metrics, f1, indent=4)\n",
    "            with open(f\"{save_path}_test_metrics.json\", \"w\") as f2:\n",
    "                json.dump(self.all_test_metrics, f2, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46043ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_transformers import DistillVisionTransformer, CorruptDistillVisionTransformer\n",
    "\n",
    "deit_small = DistillVisionTransformer(\n",
    "    EMBED_DIM, IMG_SIZE, PATCH_SIZE, NUM_CLASSES, attention_heads=NUM_HEADS,\n",
    "    num_encoders=NUM_ENCODERS, dropout=DROPOUT, drop_path=DROP_PATH\n",
    "    )\n",
    "\n",
    "Cdeit_small = CorruptDistillVisionTransformer(\n",
    "    EMBED_DIM, IMG_SIZE, PATCH_SIZE, NUM_CLASSES, attention_heads=NUM_HEADS,\n",
    "    num_encoders=NUM_ENCODERS, dropout=DROPOUT, drop_path=DROP_PATH,\n",
    "    num_img_types=NUM_IMG_TYPES, head_strategy=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa2031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 -------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'top1_acc': 0.0234375,\n",
       "  'top5_acc': 0.0703125,\n",
       "  'top1_acc_per_type': [0.032258063554763794,\n",
       "   0.0,\n",
       "   0.06896551698446274,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'top5_acc_per_type': [0.06451612710952759,\n",
       "   0.0357142873108387,\n",
       "   0.13793103396892548,\n",
       "   0.05000000074505806,\n",
       "   0.05000000074505806],\n",
       "  'error_rate_per_type': [0.9677419364452362,\n",
       "   1.0,\n",
       "   0.9310344830155373,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  'ece': 0.0004661552084144205,\n",
       "  'entropy': 4.456512451171875}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_student_module = TrainTestBaseline(deit3_small, train_batch, test_batch, NUM_IMG_TYPES, device)\n",
    "train_student_module.train(optimizer, scheduler, \"\")\n",
    "train_student_module.all_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c045913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'top1_acc': 0.015625,\n",
       "  'top5_acc': 0.0390625,\n",
       "  'top1_acc_per_type': [0.0, 0.0, 0.06896551698446274, 0.0, 0.0],\n",
       "  'top5_acc_per_type': [0.032258063554763794,\n",
       "   0.0,\n",
       "   0.1034482792019844,\n",
       "   0.0,\n",
       "   0.05000000074505806],\n",
       "  'error_rate_per_type': [1.0, 1.0, 0.9310344830155373, 1.0, 1.0],\n",
       "  'ece': 0.0006139964680187404,\n",
       "  'entropy': 4.389969348907471}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_student_module.all_test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
