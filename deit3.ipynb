{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9956ac",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8417b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Hp\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "from train_test_module import FineTuningModule, MyAugments\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c890196",
   "metadata": {},
   "source": [
    "# ------------------- Template --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f51e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed \n",
    "torch.cuda.manual_seed(22)\n",
    "random.seed(22)\n",
    "torch.manual_seed(22)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "corrupt_types = [\"brightness\", \"defocus_blur\", \"zoom_blur\", \"motion_blur\", \"fog\", \"frost\", \"snow\", \"shot_noise\", \"gaussian_noise\", \"jpeg_compression\"]\n",
    "\n",
    "# DEIT Hyper-parameters\n",
    "NUM_IMG_TYPES = len(corrupt_types)+1\n",
    "NUM_CLASSES = 10\n",
    "DROPOUT = 0\n",
    "DROP_PATH = 0.1\n",
    "\n",
    "ERASE_P = 0.25\n",
    "RANDAUG_P = 0.5\n",
    "MIXUP_P = 0.3\n",
    "CUTMIX_P = 0.3\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 50\n",
    "WARMUP_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a392cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=TensorDataset(*torch.load(\"train_cifar10.pt\", weights_only=True)), \n",
    "                                 batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(*torch.load(\"test_cifar10.pt\", weights_only=True)), \n",
    "                                 batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def set_drop_path(model, drop_path):\n",
    "    for i in range(len(model.blocks)):\n",
    "        model.blocks[i].drop_path1 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "        model.blocks[i].drop_path2 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "\n",
    "def set_dropout(model, dropout):\n",
    "    for i in range(len(model.blocks)):\n",
    "        model.blocks[i].mlp.drop1 = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        model.blocks[i].mlp.drop2 = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdab604",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb628d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5  #3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80401a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 -------\n",
      "train-loss: 3.137 -- train-acc: 0.069 -- test-loss: 3.144 -- test-acc: 0.068\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 2 -------\n",
      "train-loss: 2.283 -- train-acc: 0.273 -- test-loss: 1.540 -- test-acc: 0.514\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 3 -------\n",
      "train-loss: 1.325 -- train-acc: 0.671 -- test-loss: 0.862 -- test-acc: 0.741\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 4 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-loss: 1.066 -- train-acc: 0.774 -- test-loss: 0.677 -- test-acc: 0.796\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 5 -------\n",
      "train-loss: 0.988 -- train-acc: 0.808 -- test-loss: 0.611 -- test-acc: 0.818\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 6 -------\n",
      "train-loss: 0.950 -- train-acc: 0.825 -- test-loss: 0.576 -- test-acc: 0.829\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 7 -------\n",
      "train-loss: 0.925 -- train-acc: 0.836 -- test-loss: 0.552 -- test-acc: 0.839\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 8 -------\n",
      "train-loss: 0.907 -- train-acc: 0.843 -- test-loss: 0.536 -- test-acc: 0.846\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 9 -------\n",
      "train-loss: 0.894 -- train-acc: 0.849 -- test-loss: 0.522 -- test-acc: 0.851\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 10 -------\n",
      "train-loss: 0.883 -- train-acc: 0.853 -- test-loss: 0.512 -- test-acc: 0.856\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 11 -------\n",
      "train-loss: 0.874 -- train-acc: 0.857 -- test-loss: 0.503 -- test-acc: 0.859\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 12 -------\n",
      "train-loss: 0.867 -- train-acc: 0.860 -- test-loss: 0.497 -- test-acc: 0.862\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 13 -------\n",
      "train-loss: 0.862 -- train-acc: 0.863 -- test-loss: 0.491 -- test-acc: 0.866\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 14 -------\n",
      "train-loss: 0.857 -- train-acc: 0.865 -- test-loss: 0.485 -- test-acc: 0.867\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 15 -------\n",
      "train-loss: 0.853 -- train-acc: 0.866 -- test-loss: 0.481 -- test-acc: 0.869\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 16 -------\n",
      "train-loss: 0.850 -- train-acc: 0.868 -- test-loss: 0.479 -- test-acc: 0.871\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 17 -------\n",
      "train-loss: 0.847 -- train-acc: 0.869 -- test-loss: 0.476 -- test-acc: 0.872\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 18 -------\n",
      "train-loss: 0.845 -- train-acc: 0.870 -- test-loss: 0.473 -- test-acc: 0.873\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 19 -------\n",
      "train-loss: 0.843 -- train-acc: 0.871 -- test-loss: 0.471 -- test-acc: 0.873\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "------- Epoch 20 -------\n",
      "train-loss: 0.841 -- train-acc: 0.872 -- test-loss: 0.469 -- test-acc: 0.874\n",
      "Best model saved to deit3HEAD_all0.pth\n",
      "metrics saved!\n"
     ]
    }
   ],
   "source": [
    "deit3_small = timm.create_model('deit3_small_patch16_224.fb_in22k_ft_in1k', pretrained=True).cuda()\n",
    "deit3_small.head = nn.Linear(in_features=384, out_features=NUM_CLASSES, bias=True).cuda()\n",
    "\n",
    "optimizer = optim.SGD(deit3_small.head.parameters(), lr=lr, weight_decay=0.05)\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-3, total_iters=WARMUP_EPOCHS)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - WARMUP_EPOCHS)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, lr_scheduler], milestones=[WARMUP_EPOCHS])\n",
    "\n",
    "augmenter = MyAugments(NUM_CLASSES, mixup_p=0, cutmix_p=0, randaug_p=0, erasing_p=0)\n",
    "deit3_trainer_module = FineTuningModule(deit3_small, train_loader, test_loader, NUM_IMG_TYPES, device, freeze_body=True)\n",
    "deit3_trainer_module.train(optimizer, scheduler, augmenter, \"deit3HEAD_all0\", num_epochs=20, print_metrics=True)\n",
    "\n",
    "# all0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
